# Isomorfismo Modular en Inteligencia Artificial: Del Anillo $\mathbb{Z}/6\mathbb{Z}$ a NPUs de Arquitectura Shared-Nothing

[![License: PolyForm Noncommercial](https://img.shields.io/badge/License-PolyForm_Noncommercial_1.0.0-red.svg)](LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Platform](https://img.shields.io/badge/Platform-PyTorch_%7C_CUDA-orange.svg)](notebooks/hex_ensemble_mnist.ipynb)
[![Status](https://img.shields.io/badge/Status-Validated_(MNIST_97%25)-success.svg)]()
[![DOI](https://img.shields.io/badge/DOI-Preprint_Pending-lightgrey)](https://zenodo.org/)

**Autor:** Jos√© Ignacio Peinador Sala  
**Contacto:** [joseignacio.peinador@gmail.com](mailto:joseignacio.peinador@gmail.com)  
**ORCID:** [0009-0008-1822-3452](https://orcid.org/0009-0008-1822-3452)

---

## üìú Resumen Ejecutivo

Este repositorio contiene la **implementaci√≥n de referencia** y la validaci√≥n experimental de la arquitectura **Hex-Ensemble**, una propuesta de hardware para IA basada en el principio de **"Shared-Nothing"** (Sin Compartici√≥n).

Frente al paradigma monol√≠tico actual que enfrenta l√≠mites termodin√°micos, este trabajo propone el uso del **Isomorfismo Modular** bajo el anillo $\mathbb{Z}/6\mathbb{Z}$ para descomponer redes neuronales profundas en seis sub-redes independientes. Esto permite reemplazar las costosas interconexiones de baja latencia y las matrices de silicio de 3nm por un "enjambre" de *chiplets* econ√≥micos de 28nm, logrando una reducci√≥n de costes de **18x** sin sacrificar significativamente la precisi√≥n predictiva.

![Esquema NPU Hex-Ensemble](img/hex_npu_arch.png)
*> Esquema conceptual: Distribuci√≥n determinista de tensores mediante bus de diezmado pasivo hacia 6 n√∫cleos aislados.*

---

## üöÄ Hitos de Validaci√≥n

La arquitectura ha sido validada experimentalmente demostrando que la falta de comunicaci√≥n entre nodos (*Shared-Nothing*) act√∫a como un regularizador estructural eficiente, permitiendo un rendimiento competitivo con una fracci√≥n del coste.

| Experimento | Modelo Base | Precisi√≥n Monol√≠tica | Precisi√≥n Modular (Hex) | Gap |
| :--- | :--- | :--- | :--- | :--- |
| **Visi√≥n (CV)** | MLP (MNIST) | 98.10% | **97.03%** | -1.07% |
| **Lenguaje (NLP)** | Transformer | 100.00% | **94.75%** | -5.25% |
| **Eficiencia** | **Coste Silicio** | $666.67 (3nm) | **$37.92 (28nm)** | **-94% (18x)** |

---

## üìÇ Estructura del Repositorio

* **`src/`**: C√≥digo fuente del motor modular.
    * `hex_layers.py`: Implementaci√≥n de capas `Linear` y `Attention` con soporte nativo para Stride-6.
    * `hex_ensemble.py`: Orquestador de los 6 workers aislados y l√≥gica de *Logit Mixing*.
* **`paper/`**: Manuscrito cient√≠fico unificado (LaTeX/PDF).
    * **"Isomorfismo Modular en Inteligencia Artificial: Del Anillo $\mathbb{Z}/6\mathbb{Z}$ a NPUs de Arquitectura Shared-Nothing"**.
* **`notebooks/`**: Experimentos reproducibles.
    * `Hex_Ensemble_MNIST_Validation.ipynb`: Entrenamiento y validaci√≥n en clasificaci√≥n de d√≠gitos.
    * `Modular_Transformer_PoC.ipynb`: Prueba de concepto de atenci√≥n modular distribuida.
* **`analysis/`**: Modelos econ√≥micos.
    * `yield_wafer_model.xlsx`: Hoja de c√°lculo del modelo de costes (Yield/Wafer) comparando nodos de 28nm vs 3nm.

---

## ‚öôÔ∏è Innovaci√≥n T√©cnica

### 1. Operador de Diezmado Modular (Tensor Stride-6)
Formalizamos un operador matem√°tico $\mathcal{P}_r$ que proyecta cualquier tensor de entrada en 6 sub-espacios disjuntos bas√°ndose en la congruencia $i \equiv r \pmod 6$. Esto transforma el problema de multiplicaci√≥n de matrices densas en 6 problemas independientes de menor dimensi√≥n, isomorfos a la **Descomposici√≥n Polifase** en DSP.

### 2. Arquitectura "Shared-Nothing"
Cada uno de los 6 *Workers* opera con su propia memoria SRAM local y no tiene acceso f√≠sico a la memoria de los dem√°s. Esto elimina la necesidad de coherencia de cach√© y buses de alto ancho de banda, permitiendo una escalabilidad lineal te√≥rica ilimitada y reduciendo dr√°sticamente el consumo energ√©tico por movimiento de datos.

### 3. Arbitraje de Nodos (Economic Moat)
La arquitectura permite fabricar chips de IA de alto rendimiento utilizando nodos de litograf√≠a maduros (28nm) en lugar de nodos de vanguardia (3nm). Aprovechando el mayor *Yield* (rendimiento de oblea) de los chips peque√±os y baratos, se logra una ventaja econ√≥mica masiva frente a las GPUs monol√≠ticas.

---

## ‚öñÔ∏è Licencia y Uso (Dual Licensing)

Este proyecto utiliza un modelo de **Licenciamiento Dual** para fomentar la innovaci√≥n abierta manteniendo la sostenibilidad de la investigaci√≥n independiente.

### ‚úÖ Uso Acad√©mico y Open Source (Gratuito)
El c√≥digo fuente y la documentaci√≥n est√°n disponibles bajo la licencia **PolyForm Noncommercial License 1.0.0**.
* **Permitido:** Uso personal, investigaci√≥n acad√©mica, docencia y proyectos de c√≥digo abierto (sin √°nimo de lucro).
* **Requisito:** Debe atribuir la autor√≠a original y mantener este aviso de licencia.

### ‚õî Uso Comercial (Requiere Licencia)
**La integraci√≥n en productos propietarios, servicios de pago o hardware comercial cerrado est√° prohibida** sin un acuerdo previo. Esto incluye:
* Aceleradores de hardware (ASIC/FPGA) propietarios.
* Servicios de inferencia SaaS cerrados.
* Consultor√≠a comercial basada en esta arquitectura.

> üíº **Para obtener una Licencia Comercial (exenci√≥n de Copyleft)**, contacte con el autor: [joseignacio.peinador@gmail.com](mailto:joseignacio.peinador@gmail.com)

---

## üíª Reproducibilidad

Para replicar el experimento de MNIST en su m√°quina local:

1.  Clone el repositorio.
2.  Instale las dependencias: `pip install torch torchvision numpy`
3.  Ejecute el script de entrenamiento:
    ```bash
    python src/train_hex_mnist.py --epochs 5 --batch-size 64
    ```

---

## ‚úçÔ∏è Citaci√≥n

Si utiliza esta arquitectura, el operador Stride-6 o el an√°lisis econ√≥mico en su investigaci√≥n, por favor cite el art√≠culo original:

```bibtex
@article{PeinadorSala2025_HexNPU,
  author = {Peinador Sala, Jos√© Ignacio},
  title = {Isomorfismo Modular en Inteligencia Artificial: Del Anillo Z/6Z a NPUs de Arquitectura Shared-Nothing},
  year = {2025},
  publisher = {Zenodo},
  version = {v1.0},
  doi = {10.5281/zenodo.XXXXXXX}
}
